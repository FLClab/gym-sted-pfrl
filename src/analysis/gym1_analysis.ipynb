{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e51f170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab171c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "import matplotlib\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from matplotlib import pyplot\n",
    "from scipy.signal import windows\n",
    "\n",
    "PATH = \"../../results\"\n",
    "\n",
    "\n",
    "def calc_shape(shape, layers):\n",
    "    \"\"\"\n",
    "    Calculates the shape of the tensor after the layers\n",
    "    :param shape: A `tuple` of the input shape\n",
    "    :param layers: A `list`-like of layers\n",
    "    :returns : A `tuple` of the output shape\n",
    "    \"\"\"\n",
    "    _shape = numpy.array(shape[1:])\n",
    "    for layer in layers:\n",
    "        _shape = (_shape + 2 * numpy.array(layer.padding) - numpy.array(layer.dilation) * (numpy.array(layer.kernel_size) - 1) - 1) / numpy.array(layer.stride) + 1\n",
    "        _shape = _shape.astype(int)\n",
    "    return (shape[0], *_shape)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels=1, action_size=1, obs_size=(1, 64, 64),\n",
    "        activation=nn.functional.leaky_relu\n",
    "    ):\n",
    "        self.in_channels = in_channels\n",
    "        self.action_size = action_size\n",
    "        self.obs_size = obs_size\n",
    "        self.activation = activation\n",
    "        super(Policy, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, 16, 8, stride=4),\n",
    "            nn.Conv2d(16, 32, 4, stride=2),\n",
    "        ])\n",
    "        out_shape = calc_shape(obs_size, self.layers)\n",
    "        self.linear = nn.Linear(32 * numpy.prod(out_shape), action_size)\n",
    "        self.policy =  pfrl.policies.GaussianHeadWithStateIndependentCovariance(\n",
    "            action_size=action_size,\n",
    "            var_type=\"diagonal\",\n",
    "            var_func=lambda x: torch.exp(2 * x),  # Parameterize log std\n",
    "            var_param_init=0,  # log std = 0 => std = 1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = self.policy(x)\n",
    "        return x\n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels=1, action_size=1, obs_size=(1, 64, 64),\n",
    "        activation=torch.tanh\n",
    "    ):\n",
    "        self.in_channels = in_channels\n",
    "        self.action_size = action_size\n",
    "        self.obs_size = obs_size\n",
    "        self.activation = activation\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, 16, 8, stride=4),\n",
    "            nn.Conv2d(16, 32, 4, stride=2),\n",
    "        ])\n",
    "        out_shape = calc_shape(obs_size, self.layers)\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(32 * numpy.prod(out_shape), 64),\n",
    "            nn.Linear(64, action_size)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.linears:\n",
    "            x = self.activation(layer(x))\n",
    "        return x\n",
    "\n",
    "def sliding_window(x, kernel_size):\n",
    "    \"\"\"\n",
    "    Computes a sliding window average over the specified data\n",
    "    \n",
    "    :param x: A `numpy.ndarray` of the data\n",
    "    :param kernel_size: An `int` of the size of the sliding window\n",
    "    \n",
    "    :returns : A `numpy.ndarray` of the averaged data\n",
    "    \"\"\"\n",
    "    if kernel_size < 2:\n",
    "        return x\n",
    "    window = windows.boxcar(kernel_size)\n",
    "    window = window / window.sum()\n",
    "    _x = numpy.pad(x, (kernel_size, kernel_size), mode=\"edge\")\n",
    "    return numpy.convolve(_x, window, mode=\"same\")[kernel_size : -kernel_size]\n",
    "\n",
    "def plot_score(df, x, y, shade_keys=None, smooth=1):\n",
    "    \"\"\"\n",
    "    Plots the scores from a `pandas.DataFrame` using the provided \n",
    "    key\n",
    "    \n",
    "    :param df: A `pandas.DataFrame`\n",
    "    :param key: A `str` of the desired key\n",
    "    :param shade_keys: A `list` of keys to use as shade\n",
    "    :param xlabel:\n",
    "    \n",
    "    :returns : A `matplotlib.Figure` of the created plot\n",
    "               A `matplotlib.Axes` of the create plot\n",
    "    \"\"\"\n",
    "    fig, ax = pyplot.subplots(figsize=(3,3))\n",
    "    ax.plot(df[x], sliding_window(df[y], smooth))\n",
    "    if isinstance(shade_keys, (tuple, list)):\n",
    "        ax.fill_between(\n",
    "            df[x], \n",
    "            sliding_window(df[y] - df[shade_keys[0]], smooth), \n",
    "            sliding_window(df[y] + df[shade_keys[1]], smooth), \n",
    "            alpha=0.3\n",
    "        )\n",
    "    elif isinstance(shade_keys, str):\n",
    "        ax.fill_between(\n",
    "            df[x], \n",
    "            sliding_window(df[y] - df[shade_keys], smooth), \n",
    "            sliding_window(df[y] + df[shade_keys], smooth), \n",
    "            alpha=0.3\n",
    "        )\n",
    "    ax.set(\n",
    "        xlabel=x, ylabel=y\n",
    "    )\n",
    "    return fig, ax\n",
    "\n",
    "def savefig(fig, ax, savepath, extension=\"pdf\", save_white=False):\n",
    "    \"\"\"\n",
    "    Utilitary function allowing to save the figure to \n",
    "    the savepath\n",
    "    \n",
    "    :param fig: A `matplotlib.Figure`\n",
    "    :param ax: A `matplotlib.Axes`  \n",
    "    :param savepath: A `str` of the filename\n",
    "    :param extension: A `str` of the extension of the file\n",
    "    :param save_white: A `bool` wheter to save the figure in white version \n",
    "                       as well\n",
    "    \"\"\"\n",
    "    fig.savefig(f\"{savepath}.{extension}\", bbox_inches=\"tight\", transparent=True)\n",
    "    if save_white:\n",
    "        change_figax_color(fig, ax)\n",
    "        fig.savefig(f\"{savepath}_white.{extension}\", bbox_inches=\"tight\", transparent=True)\n",
    "        \n",
    "def change_figax_color(fig, ax):\n",
    "    \"\"\"\n",
    "    Utilitary function allowing to change the figure and \n",
    "    ax color from black to white\n",
    "    \n",
    "    :param fig: A `matplotlib.Figure`\n",
    "    :param ax: A `matplotlib.Axes`    \n",
    "    \"\"\"\n",
    "    def _change_ax(ax):\n",
    "        ax.set_facecolor(\"none\")\n",
    "        for child in ax.get_children():\n",
    "            if isinstance(child, matplotlib.spines.Spine):\n",
    "                child.set_color('white')      \n",
    "        ax.tick_params(axis='x', colors='white')\n",
    "        ax.tick_params(axis='y', colors='white')\n",
    "        ax.yaxis.label.set_color('white')\n",
    "        ax.xaxis.label.set_color('white')\n",
    "        ax.title.set_color(\"white\")\n",
    "        \n",
    "        # For line plots\n",
    "        for line in ax.get_lines():\n",
    "            if line.get_color() in [\"#000000\", \"000000\", \"black\"]:\n",
    "                line.set_color(\"white\")    \n",
    "\n",
    "        # For scatter plots\n",
    "        for collection in ax.collections:\n",
    "            new_colors = [\"white\" if matplotlib.colors.to_hex(c) == \"#000000\" else c \n",
    "                             for c in collection.get_facecolors()]\n",
    "            collection.set_facecolors(new_colors)\n",
    "            new_colors = [\"white\" if matplotlib.colors.to_hex(c) == \"#000000\" else c \n",
    "                             for c in collection.get_edgecolors()]   \n",
    "            collection.set_edgecolors(new_colors)\n",
    "\n",
    "        # For hist plots\n",
    "        for patch in ax.patches:\n",
    "            c = patch.get_facecolor()\n",
    "            if matplotlib.colors.to_hex(c) == \"#000000\":\n",
    "                patch.set_color(\"white\")        \n",
    "        \n",
    "    # Change figure background\n",
    "    fig.patch.set_facecolor(\"none\")\n",
    "    \n",
    "    # Changes colorbars if any\n",
    "    for ax in fig.axes:\n",
    "        _change_ax(ax.axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd65069",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4cccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': 'gym_sted:MOSTEDranking-hard-v4', 'num_envs': 25, 'seed': 0, 'gpu': 0, 'outdir': './data', 'exp_id': '20210901-092428', 'batchsize': 32, 'steps': 100000, 'eval_interval': 1000.0, 'eval_n_runs': 100, 'checkpoint_freq': 1000, 'reward_scale_factor': 1.0, 'render': False, 'lr': 0.0001, 'demo': False, 'load': '', 'log_level': 20, 'monitor': False, 'bleach_sampling': 'constant', 'recurrent': False}\n"
     ]
    },
    {
     "ename": "UnregisteredEnv",
     "evalue": "No registered env with id: MOSTEDranking-hard-v4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/gym-sted/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MOSTEDranking-hard-v4'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_343209/538618508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mtimestep_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mobs_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_343209/538618508.py\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m(test)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Use different random seeds for train and test envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0menv_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"env\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Use different random seeds for train and test envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gym-sted/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gym-sted/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gym-sted/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No registered env with id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m: No registered env with id: MOSTEDranking-hard-v4"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import json\n",
    "import os\n",
    "import gym_sted\n",
    "import pfrl\n",
    "import torch\n",
    "import sys\n",
    "import pandas\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "from matplotlib import pyplot\n",
    "from collections import defaultdict\n",
    "\n",
    "while \"../..\" in sys.path:\n",
    "    sys.path.remove(\"../..\")\n",
    "sys.path.insert(0, \"../..\")\n",
    "from src import models, WrapPyTorch\n",
    "\n",
    "from gym_sted.envs.sted_env import action_spaces, scales_dict, bounds_dict\n",
    "\n",
    "PATH = \"../../data\"\n",
    "model_name = \"20210901-092428_f62b76d6\"\n",
    "os.makedirs(os.path.join(PATH, model_name, \"panels\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PATH, model_name, \"eval\"), exist_ok=True)\n",
    "\n",
    "args = json.load(open(os.path.join(PATH, model_name, \"args.txt\"), \"r\"))\n",
    "print(args)\n",
    "def make_env(test):\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 42\n",
    "    env = gym.make(args[\"env\"])\n",
    "    # Use different random seeds for train and test envs\n",
    "    env.seed(env_seed)\n",
    "    # Converts the openAI Gym to PyTorch tensor shape\n",
    "    env = WrapPyTorch(env)\n",
    "    # Normalize the action space\n",
    "    env = pfrl.wrappers.NormalizeActionSpace(env)\n",
    "    return env\n",
    "\n",
    "env = make_env(True)\n",
    "timestep_limit = env.spec.max_episode_steps\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "# This happens because previous models were trained using other version \n",
    "# of the models\n",
    "try:\n",
    "    policy = models.Policy2(action_size=action_space.shape[0], obs_space=obs_space)\n",
    "    vf = models.ValueFunction2(obs_space=obs_space)\n",
    "    model = pfrl.nn.Branched(policy, vf)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "    agent = pfrl.agents.PPO(\n",
    "        model,\n",
    "        opt,\n",
    "        gpu=None,\n",
    "        minibatch_size=args[\"batchsize\"],\n",
    "        max_grad_norm=1.0,\n",
    "        update_interval=100\n",
    "    )\n",
    "    agent.load(os.path.join(PATH, model_name, \"best\"))\n",
    "except RuntimeError:\n",
    "    policy = Policy(action_size=action_space.shape[0], obs_size=obs_space.shape)\n",
    "    vf = ValueFunction(obs_size=obs_space.shape)\n",
    "    model = pfrl.nn.Branched(policy, vf)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "    agent = pfrl.agents.PPO(\n",
    "        model,\n",
    "        opt,\n",
    "        gpu=None,\n",
    "        minibatch_size=args[\"batchsize\"],\n",
    "        max_grad_norm=1.0,\n",
    "        update_interval=512\n",
    "    )\n",
    "    agent.load(os.path.join(PATH, model_name, \"best\"))\n",
    "    \n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(os.path.join(PATH, model_name, \"scores.txt\"), sep=\"\\t\")\n",
    "display(df.head())\n",
    "\n",
    "smoothing_factor = 0\n",
    "fig, ax = plot_score(df, \"steps\", \"mean\", shade_keys=\"stdev\", smooth=smoothing_factor)\n",
    "savefig(fig, ax, os.path.join(PATH, model_name, \"panels\", \"mean_reward\"), save_white=True)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "phy_reacts = {\n",
    "    \"low-bleach\" : gym_sted.defaults.FLUO[\"phy_react\"],\n",
    "    \"mid-bleach\" : {488: 0.5e-7 + 3 * 0.25e-7, 575: 50.0e-11 + 3 * 25.0e-11},\n",
    "    \"high-bleach\" : {488: 0.5e-7 + 10 * 0.25e-7, 575: 50.0e-11 + 10 * 25.0e-11},\n",
    "}\n",
    "episode_count = 10\n",
    "render, done = False, False\n",
    "\n",
    "episode_stats = defaultdict(list)\n",
    "for key, phy_react in phy_reacts.items():\n",
    "    with agent.eval_mode():\n",
    "        for i in trange(episode_count, desc=key, leave=False):\n",
    "            observation = env.reset()\n",
    "\n",
    "            # Sets the microscope bleach constant to default values\n",
    "            env.microscope.fluo.phy_react = phy_react\n",
    "    #         env.synapse_generator = gym_sted.utils.SynapseGenerator(mode=\"rand\", seed=None, molecules=1)\n",
    "    #         env.microscope.fluo.phy_react = {\n",
    "    #             488: 0.5e-7 + 3 * 0.25e-7,\n",
    "    #             575: 50.0e-11 + 3 * 25.0e-11\n",
    "    #         }\n",
    "\n",
    "            timestep, episode_len, cum_rewards = 0, 0, 0\n",
    "            max_episode_len = env.spec.max_episode_steps\n",
    "\n",
    "            stats = defaultdict(list)        \n",
    "            while True:\n",
    "                action = agent.act(observation)\n",
    "\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                reset = done or episode_len == max_episode_len or info.get(\"needs_reset\", False)\n",
    "                agent.observe(observation, reward, done, reset)\n",
    "\n",
    "                stats[\"reward\"].append(reward)\n",
    "                stats[\"info\"].append(info)\n",
    "\n",
    "                if render:\n",
    "                    env.render(info)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                episode_len += 1\n",
    "\n",
    "            episode_stats[key].append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(episode_stats, open(os.path.join(PATH, model_name, \"eval\", \"stats.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d0125",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_avg_reward(episode_stats):\n",
    "    values = [sum(stats[\"reward\"]) for stats in episode_stats]\n",
    "    fig, ax = pyplot.subplots(figsize=(3,3))\n",
    "    ax.violinplot(values)\n",
    "    ax.set(\n",
    "        ylim=(-0.1, max(1.1, max(values))), title=\"Average reward\"\n",
    "    )\n",
    "    return fig, ax\n",
    "\n",
    "def plot_avg_rewards(episode_stats):\n",
    "    values = [stats[\"info\"][0][\"rewards\"] for stats in episode_stats]\n",
    "    values = numpy.array(values)    \n",
    "\n",
    "    fig, ax = pyplot.subplots(figsize=(3,3))\n",
    "    for i, (key, value) in enumerate(zip(env.obj_names, values.T)):    \n",
    "        value = (value - scales_dict[key][\"min\"]) / (scales_dict[key][\"max\"] - scales_dict[key][\"min\"])\n",
    "        ax.violinplot(value, positions=[i])\n",
    "        bound = bounds_dict[key][\"max\"] if key != \"SNR\" else bounds_dict[key][\"min\"]\n",
    "        bound = (bound - scales_dict[key][\"min\"]) / (scales_dict[key][\"max\"] - scales_dict[key][\"min\"])\n",
    "        ax.scatter(i, bound, marker=\"_\", s=50, color=\"black\")\n",
    "    ax.set(\n",
    "        ylim=(0, 1), title=\"Objective\", \n",
    "        xticks=(numpy.arange(values.shape[-1])), xticklabels=(env.obj_names)\n",
    "    )\n",
    "    return fig, ax\n",
    "    \n",
    "def plot_avg_action(episode_stats):\n",
    "    values = [stats[\"info\"][0][\"action\"] for stats in episode_stats]\n",
    "    values = numpy.array(values)\n",
    "    \n",
    "    fig, ax = pyplot.subplots(figsize=(3,3))\n",
    "    for i, (key, value) in enumerate(zip(env.actions, values.T)):    \n",
    "        value = (value - action_spaces[key][\"low\"]) / (action_spaces[key][\"high\"] - action_spaces[key][\"low\"])\n",
    "        ax.violinplot(value, positions=[i])\n",
    "    ax.set(\n",
    "        ylim=(0, 1), title=\"Actions\",\n",
    "        xticks=(numpy.arange(values.shape[-1])), xticklabels=(env.actions)        \n",
    "    )\n",
    "    return fig, ax\n",
    "\n",
    "def plot_avg_multiaction(episode_stats):\n",
    "    values = [numpy.array([step[\"action\"] for step in stats[\"info\"]]) for stats in episode_stats]\n",
    "\n",
    "    out = {\"imaging-action\" : []}\n",
    "    for i, key in enumerate(env.actions):\n",
    "        fig, ax = pyplot.subplots(figsize=(3,3))\n",
    "        for value in values:\n",
    "            value = (value[:, i] - action_spaces[key][\"low\"]) / (action_spaces[key][\"high\"] - action_spaces[key][\"low\"])\n",
    "            ax.plot(value, alpha=0.3, color=\"black\", label=key)\n",
    "            ax.scatter(len(value) - 1, value[-1], marker=\"*\", color=\"black\")\n",
    "        ax.set(\n",
    "            ylim=(0, 1), title=\"Imaging action\",\n",
    "            xlabel=\"Steps\", ylabel=key\n",
    "        )\n",
    "        out[\"imaging-action\"].append((fig, ax))\n",
    "    \n",
    "    fig, ax = pyplot.subplots(figsize=(3,3))\n",
    "    for value in values:\n",
    "        value = numpy.clip(value[:, -1].astype(int) - 1, 0, 2)\n",
    "        index = numpy.argmax(value == 1)\n",
    "        where = value == 1\n",
    "        where[index] = False\n",
    "        value[where] = 0\n",
    "        value[-1] = 2\n",
    "        ax.plot(value, color=\"black\", alpha=0.3)\n",
    "    ax.set(\n",
    "        ylim=(0, 2), title=\"Main action\",\n",
    "        xlabel=\"Steps\"\n",
    "    )\n",
    "    out[\"main-action\"] = (fig, ax)    \n",
    "    \n",
    "    return out\n",
    "\n",
    "def plot_last_image(episode_stats):\n",
    "    \"\"\"\n",
    "    Plots the last acquired image across all episodes\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    images = [stats[\"info\"][-1][\"conf1\"] for stats in episode_stats]\n",
    "    fig, axes = pyplot.subplots(3, 3, figsize=(10,10), sharex=True, sharey=True)\n",
    "    for ax, image in zip(axes.ravel(), images):\n",
    "        ax.imshow(image, cmap=\"hot\", vmin=0, vmax=1000)\n",
    "    out[\"conf1\"] = (fig, axes)\n",
    "        \n",
    "    images = [stats[\"info\"][-1][\"sted_image\"] for stats in episode_stats]\n",
    "    fig, axes = pyplot.subplots(3, 3, figsize=(10,10), sharex=True, sharey=True)\n",
    "    vmax = max([img.max() for img in images])\n",
    "    for ax, image in zip(axes.ravel(), images):\n",
    "        ax.imshow(image, cmap=\"hot\", vmin=0, vmax=vmax) \n",
    "    out[\"sted_image\"] = (fig, axes)        \n",
    "        \n",
    "    images = [stats[\"info\"][-1][\"conf2\"] for stats in episode_stats]\n",
    "    fig, axes = pyplot.subplots(3, 3, figsize=(10,10), sharex=True, sharey=True)\n",
    "    for ax, image in zip(axes.ravel(), images):\n",
    "        ax.imshow(image, cmap=\"hot\", vmin=0, vmax=1000)                \n",
    "    out[\"conf2\"] = (fig, axes)            \n",
    "    return out\n",
    "\n",
    "for bleach in phy_reacts.keys():\n",
    "\n",
    "    fig, ax = plot_avg_reward(episode_stats[bleach])\n",
    "    savefig(fig, ax, os.path.join(PATH, model_name, \"panels\", f\"{bleach}_avg_reward\"), save_white=True)\n",
    "\n",
    "    # fig, ax = plot_avg_rewards(episode_stats)\n",
    "    # savefig(fig, ax, os.path.join(PATH, model_name, \"panels\", \"avg_rewards\"), save_white=True)\n",
    "\n",
    "    # fig, ax = plot_avg_action(episode_stats)\n",
    "    # savefig(fig, ax, os.path.join(PATH, model_name, \"panels\", \"avg_action\"), save_white=True)\n",
    "\n",
    "    figaxes = plot_avg_multiaction(episode_stats[bleach])\n",
    "    for key, figaxes in figaxes.items():\n",
    "        if isinstance(figaxes, tuple):\n",
    "            fig, ax = figaxes\n",
    "            savefig(fig, ax, os.path.join(PATH, model_name, \"panels\", f\"{bleach}_avg_multiaction_{key}\"), save_white=True)\n",
    "        else:\n",
    "            for action_name, (fig, ax) in zip(env.actions, figaxes):\n",
    "                savefig(fig, ax, os.path.join(PATH, model_name, \"panels\", f\"{bleach}_avg_multiaction_{key}_{action_name}\"), save_white=True)            \n",
    "\n",
    "    figaxes = plot_last_image(episode_stats[bleach])\n",
    "    for key, figaxes in figaxes.items():\n",
    "        if isinstance(figaxes, tuple):\n",
    "            fig, ax = figaxes\n",
    "            savefig(fig, ax, os.path.join(PATH, model_name, \"panels\", f\"{bleach}_last-images_{key}\"), save_white=True)\n",
    "        else:\n",
    "            for action_name, (fig, ax) in zip(env.actions, figaxes):\n",
    "                savefig(fig, ax, os.path.join(PATH, model_name, \"panels\", f\"{bleach}_last-images_{key}_{action_name}\"), save_white=True)            \n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78322e39",
   "metadata": {},
   "source": [
    "# Analyse results - Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49db0ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import functools\n",
    "\n",
    "def _batch_run_episodes_record(\n",
    "    env,\n",
    "    agent,\n",
    "    n_steps,\n",
    "    n_episodes,\n",
    "    max_episode_len=None,\n",
    "    logger=None,\n",
    "):\n",
    "    \"\"\"Run multiple episodes and return returns in a batch manner.\"\"\"\n",
    "    assert (n_steps is None) != (n_episodes is None)\n",
    "\n",
    "    logger = logger or logging.getLogger(__name__)\n",
    "    num_envs = env.num_envs\n",
    "    episode_returns = dict()\n",
    "    episode_lengths = dict()\n",
    "    episode_infos = dict()\n",
    "    episode_indices = numpy.zeros(num_envs, dtype=\"i\")\n",
    "    episode_idx = 0\n",
    "    for i in range(num_envs):\n",
    "        episode_indices[i] = episode_idx\n",
    "        episode_idx += 1\n",
    "    episode_r = numpy.zeros(num_envs, dtype=numpy.float64)\n",
    "    episode_len = numpy.zeros(num_envs, dtype=\"i\")\n",
    "    episode_info = [[] for _ in range(num_envs)]\n",
    "\n",
    "    obss = env.reset()\n",
    "    rs = numpy.zeros(num_envs, dtype=\"f\")\n",
    "\n",
    "    termination_conditions = False\n",
    "    timestep = 0\n",
    "    while True:\n",
    "        \n",
    "        # a_t\n",
    "        actions = agent.batch_act(obss)\n",
    "        timestep += 1\n",
    "        # o_{t+1}, r_{t+1}\n",
    "        obss, rs, dones, infos = env.step(actions)\n",
    "        episode_r += rs\n",
    "        episode_len += 1\n",
    "        for i, info in enumerate(infos):\n",
    "            episode_info[i].append(info)\n",
    "\n",
    "        # Compute mask for done and reset\n",
    "        if max_episode_len is None:\n",
    "            resets = numpy.zeros(num_envs, dtype=bool)\n",
    "        else:\n",
    "            resets = episode_len == max_episode_len\n",
    "        resets = numpy.logical_or(\n",
    "            resets, [info.get(\"needs_reset\", False) for info in infos]\n",
    "        )\n",
    "\n",
    "        # Make mask. 0 if done/reset, 1 if pass\n",
    "        end = numpy.logical_or(resets, dones)\n",
    "        not_end = numpy.logical_not(end)\n",
    "\n",
    "        for index in range(len(end)):\n",
    "            if end[index]:\n",
    "                episode_returns[episode_indices[index]] = episode_r[index]\n",
    "                episode_lengths[episode_indices[index]] = episode_len[index]\n",
    "                episode_infos[episode_indices[index]] = episode_info[index]\n",
    "                # Give the new episode an a new episode index\n",
    "                episode_indices[index] = episode_idx\n",
    "                episode_idx += 1\n",
    "\n",
    "        # Resets done episode\n",
    "        episode_r[end] = 0\n",
    "        episode_len[end] = 0\n",
    "        for index in range(len(end)):\n",
    "            if end[index]:\n",
    "                episode_info[index] = []\n",
    "\n",
    "        # find first unfinished episode\n",
    "        first_unfinished_episode = 0\n",
    "        while first_unfinished_episode in episode_returns:\n",
    "            first_unfinished_episode += 1\n",
    "\n",
    "        # Check for termination conditions\n",
    "        eval_episode_returns = []\n",
    "        eval_episode_lens = []\n",
    "        eval_episode_infos = []\n",
    "        if n_steps is not None:\n",
    "            total_time = 0\n",
    "            for index in range(first_unfinished_episode):\n",
    "                total_time += episode_lengths[index]\n",
    "                # If you will run over allocated steps, quit\n",
    "                if total_time > n_steps:\n",
    "                    break\n",
    "                else:\n",
    "                    eval_episode_returns.append(episode_returns[index])\n",
    "                    eval_episode_lens.append(episode_lengths[index])\n",
    "                    eval_episode_infos.append(episode_infos[index])\n",
    "            termination_conditions = total_time >= n_steps\n",
    "            if not termination_conditions:\n",
    "                unfinished_index = numpy.where(\n",
    "                    episode_indices == first_unfinished_episode\n",
    "                )[0]\n",
    "                if total_time + episode_len[unfinished_index] >= n_steps:\n",
    "                    termination_conditions = True\n",
    "                    if first_unfinished_episode == 0:\n",
    "                        eval_episode_returns.append(episode_r[unfinished_index])\n",
    "                        eval_episode_lens.append(episode_len[unfinished_index])\n",
    "                        eval_episode_infos.append(episode_infos[index])\n",
    "        else:\n",
    "            termination_conditions = first_unfinished_episode >= n_episodes\n",
    "            if termination_conditions:\n",
    "                # Get the first n completed episodes\n",
    "                for index in range(n_episodes):\n",
    "                    eval_episode_returns.append(episode_returns[index])\n",
    "                    eval_episode_lens.append(episode_lengths[index])\n",
    "                    eval_episode_infos.append(episode_infos[index])                    \n",
    "\n",
    "        if termination_conditions:\n",
    "            # If this is the last step, make sure the agent observes reset=True\n",
    "            resets.fill(True)\n",
    "\n",
    "        # Agent observes the consequences.\n",
    "        agent.batch_observe(obss, rs, dones, resets)\n",
    "\n",
    "        if termination_conditions:\n",
    "            break\n",
    "        else:\n",
    "            obss = env.reset(not_end)\n",
    "\n",
    "    for i, (epi_len, epi_ret) in enumerate(\n",
    "        zip(eval_episode_lens, eval_episode_returns)\n",
    "    ):\n",
    "        logger.info(\"evaluation episode %s length: %s R: %s\", i, epi_len, epi_ret)\n",
    "    scores = [float(r) for r in eval_episode_returns]\n",
    "    lengths = [float(ln) for ln in eval_episode_lens]\n",
    "    infos = [info for info in eval_episode_infos]\n",
    "    return scores, lengths, infos\n",
    "\n",
    "\n",
    "def batch_run_evaluation_episodes_record_actions(\n",
    "    env,\n",
    "    agent,\n",
    "    n_steps,\n",
    "    n_episodes,\n",
    "    max_episode_len=None,\n",
    "    logger=None,\n",
    "):\n",
    "    \"\"\"Run multiple evaluation episodes and return returns in a batch manner.\n",
    "\n",
    "    Args:\n",
    "        env (VectorEnv): Environment used for evaluation.\n",
    "        agent (Agent): Agent to evaluate.\n",
    "        n_steps (int): Number of total timesteps to evaluate the agent.\n",
    "        n_episodes (int): Number of evaluation runs.\n",
    "        max_episode_len (int or None): If specified, episodes\n",
    "            longer than this value will be truncated.\n",
    "        logger (Logger or None): If specified, the given Logger\n",
    "            object will be used for logging results. If not\n",
    "            specified, the default logger of this module will\n",
    "            be used.\n",
    "\n",
    "    Returns:\n",
    "        List of returns of evaluation runs.\n",
    "    \"\"\"\n",
    "    with agent.eval_mode():\n",
    "        return _batch_run_episodes_record(\n",
    "            env=env,\n",
    "            agent=agent,\n",
    "            n_steps=n_steps,\n",
    "            n_episodes=n_episodes,\n",
    "            max_episode_len=max_episode_len,\n",
    "            logger=logger,\n",
    "        )\n",
    "    \n",
    "PATH = \"../../data\"\n",
    "model_name = \"20210901-092428_f62b76d6\"\n",
    "os.makedirs(os.path.join(PATH, model_name, \"panels\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PATH, model_name, \"eval\"), exist_ok=True)\n",
    "\n",
    "args = json.load(open(os.path.join(PATH, model_name, \"args.txt\"), \"r\"))\n",
    "args[\"env\"] = \"gym_sted:MOSTEDRankingWithArticulation-hard-v4\"\n",
    "NUM_ENVS = 5\n",
    "\n",
    "process_seeds = numpy.arange(NUM_ENVS) + 42\n",
    "def make_env(idx, test):\n",
    "    # Use different random seeds for train and test envs\n",
    "    process_seed = int(process_seeds[idx])\n",
    "    env_seed = 2 ** 32 - 1 - process_seed if test else process_seed\n",
    "    env = gym.make(args[\"env\"])\n",
    "    # Use different random seeds for train and test envs\n",
    "    env.seed(env_seed)\n",
    "    # Converts the openAI Gym to PyTorch tensor shape\n",
    "    env = WrapPyTorch(env)\n",
    "    # Normalize the action space\n",
    "    env = pfrl.wrappers.NormalizeActionSpace(env)\n",
    "    return env\n",
    "\n",
    "def make_batch_env(test):\n",
    "    vec_env = pfrl.envs.MultiprocessVectorEnv(\n",
    "        [\n",
    "            functools.partial(make_env, idx, test)\n",
    "            for idx, env in enumerate(range(NUM_ENVS))\n",
    "        ]\n",
    "    )\n",
    "    # vec_env = pfrl.wrappers.VectorFrameStack(vec_env, 4)\n",
    "    return vec_env\n",
    "\n",
    "env = make_env(0, True)\n",
    "timestep_limit = env.spec.max_episode_steps\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "# Creates the agent \n",
    "policy = models.PolicyWithSideAction(action_size=action_space.shape[0], obs_space=obs_space)\n",
    "vf = models.ValueFunction2(obs_space=obs_space)\n",
    "model = pfrl.nn.Branched(policy, vf)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "agent = pfrl.agents.PPO(\n",
    "    model,\n",
    "    opt,\n",
    "    gpu=None,\n",
    "    minibatch_size=args[\"batchsize\"],\n",
    "    max_grad_norm=1.0,\n",
    "    update_interval=512\n",
    ")\n",
    "agent.load(os.path.join(PATH, model_name, \"best\"))\n",
    "    \n",
    "# Creates the batch envs\n",
    "env = make_batch_env(test=True)\n",
    "\n",
    "scores, lengths, records = batch_run_evaluation_episodes_record_actions(env, agent, n_steps=None, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb4f798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(records))\n",
    "print(len(records[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd484a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-sted",
   "language": "python",
   "name": "gym-sted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
